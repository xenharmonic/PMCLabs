{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: IIR filters, FFT Windowing, Effects\n",
    "\n",
    "### 12 March 2018\n",
    "\n",
    "# Goals #\n",
    "\n",
    "After doing this lab, you should be able to:\n",
    "* Use an iir filter design tool to create a filter, then apply it to a sound\n",
    "* Have hands-on experience using windowing for better FFT analysis and effects\n",
    "* Understand how STFT can be used to implement effects in the frequency domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: IIR Filter design #\n",
    "\n",
    "In this section, you will see how to use an IIR filter design tool, and compare it to an FIR filter.\n",
    "\n",
    "## Loading some sound files ##\n",
    "a. Start by grabbing the same music files you used last week. Put them in the same directory as this lab.\n",
    "\n",
    "* http://www.doc.gold.ac.uk/~mas01rf/PMC2014-15/IPython/lab13/song1.wav\n",
    "* http://www.doc.gold.ac.uk/~mas01rf/PMC2016-17/lab17/song2.wav\n",
    "\n",
    "These are free sounds downloaded from\n",
    "\n",
    "http://freemusicarchive.org/music/Jahzzar/Travellers_Guide/Siesta\n",
    "\n",
    "and\n",
    "\n",
    "http://freemusicarchive.org/music/Black_Ant/Free_Beats_Sel_3/Fater_Lee\n",
    "\n",
    "b. Now load them into variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song1 = wavReadMono(\"song1.wav\")\n",
    "song2 = wavReadMono(\"song2.wav\")\n",
    "#Listen to them if you'd like:\n",
    "play(song1)\n",
    "play(song2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an alarm sound ## \n",
    "c. Use the code below (same as last week) to add an \"alarm\" sound to song1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This code adds a 5000Hz sine wave to the first 1 second of song1, then stores it in a new variable:\n",
    "t = np.arange(0, 1, 1/44100)\n",
    "song1_5000 = song1[0:44100] + 0.3 * sin(2*pi*5000*t)\n",
    "play(song1) #normal song\n",
    "play(song1_5000) #song with alarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the normal song, and the song with the alarm added, above. Make sure you can hear the alarm.\n",
    "\n",
    "## Getting started with the filter design tool ## \n",
    "d. The `signal.iirfilter` function will create you a new IIR filter. At minimum, you need to tell it the order of the filter (i.e., how many feedforward and feedback coefficients), and the cutoff frequency (or frequencies, in the case of a bandpass/bandstop filter).\n",
    "\n",
    "The cutoff frequency is expressed as a proportion of the Nyquist rate. For instance, a cutoff of 0.5 would be 1/2 the Nyquist rate, which would correspond to 11025Hz if you have a sample rate of 44100Hz (and therefore a Nyquist rate of 22050 Hz).\n",
    "\n",
    "Use the `?` syntax below to read more about this filter design function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?signal.iirfilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns two coefficient arrays, b and a. Just like you saw in lecture, b contains the feedforward coefficients, and a contains the feedback coefficients. \n",
    "\n",
    "For example, the following code creates a highpass filter with order 10, with a cutoff at 0.3 times the Nyquist frequency. Then it prints out the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, a = signal.iirfilter(10, 0.3, btype='highpass')\n",
    "print \"b is \", b\n",
    "print \"a is \", a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can copy these coefficients into your own code in any environment, and use the IIR filter equation to compute your filter!\n",
    "\n",
    "But first you probably want to look at the frequency response to see what it looks like.\n",
    "\n",
    "The `signal.freqz` function below computes the frequency response for frequencies from 0 to the Nyquist. It stores the frequency response in h and the frequencies in w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w, h = signal.freqz(b, a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to plot the response, and we can re-use this function later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotResponse(w, h):\n",
    "    fig = plt.figure()\n",
    "    plt.title('Digital filter frequency response')\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    plt.plot(w, 20 * np.log10(abs(h)), 'b') #plot in dB scale, wow!\n",
    "    plt.ylabel('Amplitude [dB]', color='b')\n",
    "    plt.xlabel('Frequency [rad/sample]')\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    angles = np.unwrap(np.angle(h))\n",
    "    plt.plot(w, angles, 'g')\n",
    "    plt.ylabel('Angle (radians)', color='g')\n",
    "    plt.grid()\n",
    "    plt.axis('tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the response, using w and h:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotResponse(w,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply this filter to a sound, you can use the lfilter function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sound = signal.lfilter(b, a, original_sound) #make sure you load a sound in original_sound first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the filter design tool ##\n",
    "\n",
    "e. Now, use the space below to create a filter to filter out the alarm sound from `song1_5000`. Use visual inspection of the frequency response and your ears to figure out what a good filter will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Your work here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections ##\n",
    "\n",
    "How does this IIR filter compare to the FIR filter you made in last week's lab? How many coefficients do you need to remove the alarm sound? Is it more or less than the FIR filter? How much delay is introduced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Type your reflections here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: FFT and Windows #\n",
    "\n",
    "Recall from lecture:\n",
    "\n",
    "1. When you select a finite number of audio samples and take the FFT, you are implicitly applying a *rectangular window* to these samples. That is, you are multiplying your audio by a signal whose value is 0 for a while, then 1 for a while, then 0 for a while.\n",
    "2. When you multiply two signals in the time domain, this is equivalent to convolving them in the frequency domain.\n",
    "3. The spectrum of the rectangular window is ugly and bumpy (i.e., it has high side lobes). This will lead to the FFT of your windowed sound having bumps as well, making your FFT harder to interpret.\n",
    "4. Therfore, we can get better results by multiplying the sound to be analysed with another type of window, whose spectrum is \"nicer.\"\n",
    "\n",
    "The functions below can be used to generate \"nice\" windows of any length. Read their help files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?np.hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?np.hanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?np.blackman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these functions to create three window of 1024 samples each. Then plot each window to take a look at the function that is generated. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two 1-second sine wave sampled at 100 Hz. The first sine should have a frequency which will exactly match a frequency bin of a 100-point FFT. The second should have a frequency which will *not* exactly match a frequency bin.\n",
    "\n",
    "(Recall that a 100-point FFT will give you 100 bins. The frequency of the first bin is 0, and the bins evenly divide the frequency space from 0 to the sampling rate into 100 bins.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = # make your time array\n",
    "s1 = #your first sine wave\n",
    "s2 = #your second sine wave"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the FFT of s1 and s2. How do their shapes compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot your FFTs here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the following code to apply a 100-point Hamming window to s2, and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_windowed = np.hamming(100)*s2\n",
    "plot(s2_windowed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the FFT of s2_windowed. How does it compare to the FFT of s2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot your FFTs here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try computing the FFT of a short snippet of sound from song1 or song2. Compare the FFT with and without windowing. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Using the STFT to implement effects #\n",
    "\n",
    "a. Use Audacity or anothe program to record yourself saying a few words into an audio file, and save this file as \"me.wav\", in the same directory as this sketch.\n",
    "\n",
    "Use the code below to load this file into a Python array and play it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "me = wavReadMono(\"me.wav\")\n",
    "play(me)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below performs cross synthesis between your voice and song1. Specifically:\n",
    "\n",
    "* For each 2048-sample analysis frame in song1:\n",
    "    * Take an FFT analysis frame from song1, and call this f1\n",
    "    * Take an FFT analysis frame from song2, and call this f2\n",
    "    * Compute a new spectrum of the same size, by multiplying each (complex) bin of f1 with the magnitude of the same bin of f2\n",
    "    * Take the IFFT of this spectrum to produce 2048 samples of audio\n",
    "    * Add this audio to an audio buffer\n",
    "    * Then move 16 samples forward in time within song1, within your recording, and within the buffer, repeating the process until you reach the end of the song1 file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSound = zeros(size(song1)) #make an audio \"buffer\" the same length as song1\n",
    "i = 0 #our index in song1 \n",
    "j = 0; #our index in \"me\"\n",
    "width = 2048; #width of our anlaysis frame\n",
    "hop = int(2048/16); #hop between subsequent frames\n",
    "win = np.hamming(2048) #our window :) \n",
    "\n",
    "while (i + width < size(song1)) :  #Do this until we get to the end of song1\n",
    "    if (j + width >= size(me)) : #if we reach the end of \"me\" first, start at the beginning\n",
    "        j = 0\n",
    "    frame1 = win*song1[i:i+width] #select the next 'width' samples from song1 and window them\n",
    "    frame2 = win*me[j:j+width] #select the next 'width' samples from me and window them\n",
    "    f1 = fft.fft(frame1) #the next fft of song1 \n",
    "    f2 = fft.fft(frame2) #the next fft of \"me\"\n",
    "    magnitudes = abs(f2) #the magnitudes of \"me\"\n",
    "    frame3 = f1 * magnitudes #multiply the complex fft, f1, by the magnitudes of \"me\", bin by bin\n",
    "        \n",
    "    sig1 = fft.ifft(frame3).real #take the inverse FFT to generate audio, and throw away any imaginary garbage, keeping only the real part\n",
    "    newSound[i:i+width] = newSound[i:i+width] + sig1 #add this to the new sound buffer\n",
    "    i = int(i + hop) #move to the next analysis location in song1 \n",
    "    j = int(j + hop) #move to the next analysis location in \"me\"\n",
    "\n",
    "newSound = 0.7*newSound/(max(abs(newSound))) #normalise the audio file so it doesn't clip\n",
    "play(newSound) #play it!\n",
    "plot(newSound) #plot it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you understand the code above. Try changing it! For instance:\n",
    "* What happens if you use the magnitude of \"song1\" and the complex FFT of \"me\" (i.e., switching their roles?)\n",
    "* Try using two different audio inputs (e.g., two songs, or a voice with white noise, or....)\n",
    "\n",
    "Think about how you might change this code to make the new sound twice as long, doing time-stretching at the same time.... :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
